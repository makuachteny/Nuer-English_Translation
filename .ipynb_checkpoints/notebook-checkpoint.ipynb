{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nuer-English_Translation\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "The goal of this project is to create a translation model that translate english to nuer language(my native language). The model is going to translate a small set of data.\n",
    "\n",
    "## Approach\n",
    "\n",
    "To translate english to nuer, we need to build a recurrent neural network(RNN). To build the RNN pipeline we need to start by:\n",
    "1. **preprocessing**; Load and examine the data, clean, tokenize and pad it.\n",
    "2. **Modeling**; build, train, and test the model\n",
    "3. **Prediction**; Create specific translations of english to Nuer, and then compare the output translations to the ground truth translations.\n",
    "4. **Iteration**; Go through the model, experimenting with different architectures.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary packages and libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install the packages and libraries\n",
    "%pip install numpy\n",
    "%pip install tensorflow \n",
    "%pip install keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import load_func\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model, Sequential\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.17.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/makuachtenygatluak/Documents/alu-machine_learning/Nuer-English_Translation/transenv/bin/python\n",
      "['/Library/Frameworks/Python.framework/Versions/3.12/lib/python312.zip', '/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12', '/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/lib-dynload', '', '/Users/makuachtenygatluak/Documents/alu-machine_learning/Nuer-English_Translation/transenv/lib/python3.12/site-packages', '/Users/makuachtenygatluak/Documents/alu-machine_learning/Nuer-English_Translation/transenv/lib/python3.12/site-packages/setuptools/_vendor']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "- I will go through the dataset and clean it if necessary. I have two datasets, the english.txt and the nuer.txt files. Each line in the english.txt file has a respective translation in each line of nuer.txt. I created a function outside the notebook to load the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the data\n",
    "english_sentences = load_func.load_data('data/english.txt')\n",
    "nuer_sentences = load_func.load_data('data/nuer.txt')\n",
    "\n",
    "print(english_sentences[0])\n",
    "\n",
    "\n",
    "# Check the corresponding sentences\n",
    "for i in range(5):\n",
    "    print(\"English sample: \", english_sentences[i])\n",
    "    print(\"Nuer samples: \",nuer_sentences[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess\n",
    "- Convert the text into a sequences of integers using:\n",
    "1. Tokenization of the words into ids\n",
    "2. Adding padding to make all the sequences the samee length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tokenize the data\n",
    "def tokenize(x):\n",
    "    \"\"\" Tokenize x\n",
    "        : param x: List of sentences/strings to be tokenized\n",
    "        : return: Tuple of (tokenized x data, tokenizer used to tokenize x) \n",
    "    \"\"\"\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(x)\n",
    "    return tokenizer.texts_to_sequences(x), tokenizer\n",
    "\n",
    "\n",
    "text_sentences, text_tokenizer = tokenize(english_sentences)\n",
    "# print(text_sentences[0])\n",
    "\n",
    "for i, (sent, token_sent) in enumerate(zip(english_sentences, text_sentences)):\n",
    "    print('Sequence {} in x'.format(i + 1))\n",
    "    print('Original sentence:', sent)\n",
    "    print('Tokenized sentence:', token_sent)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad the data\n",
    "def pad(x, length=None):\n",
    "    \"\"\" Pad x\n",
    "        : param x: List of sequences.\n",
    "        : param length: Length to pad the sequence to.  If\n",
    "    \"\"\"\n",
    "    \n",
    "    return pad_sequences(x, maxlen=length, padding='post')\n",
    "text_sentences_padded = pad(text_sentences)\n",
    "for i, (token_sent, pad_sent) in enumerate(zip(text_sentences, text_sentences_padded)):\n",
    "    print('Sequence {} in x'.format(i + 1))\n",
    "    print('Original sentence:', token_sent)\n",
    "    print('Padded sentence:', pad_sent)\n",
    "    print()   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the data\n",
    "def preprocess(x, y):\n",
    "    \"\"\" Preprocess x and y\n",
    "        : param x: Feature List of sentences\n",
    "        : param y: Label List of sentences\n",
    "        : return: Tuple of (Preprocessed x, Preprocessed y, x tokenizer, y tokenizer)\n",
    "    \"\"\"\n",
    "    preprocess_x, x_tk = tokenize(x)\n",
    "    preprocess_y, y_tk = tokenize(y)\n",
    "\n",
    "    preprocess_x = pad(preprocess_x)\n",
    "    preprocess_y = pad(preprocess_y)\n",
    "\n",
    "    # Keras's sparse_categorical_crossentropy function requires the labels to be in 3 dimensions\n",
    "    preprocess_y = preprocess_y.reshape(*preprocess_y.shape, 1)\n",
    "\n",
    "    return preprocess_x, preprocess_y, x_tk, y_tk\n",
    "\n",
    "preproc_english_sentences, preproc_nuer_sentences, english_tokenizer, nuer_tokenizer = preprocess(english_sentences, nuer_sentences)\n",
    "\n",
    "max_english_sequence_length = preproc_english_sentences.shape[1]\n",
    "max_nuer_sequence_length = preproc_nuer_sentences.shape[1]\n",
    "english_vocab_size = len(english_tokenizer.word_index)\n",
    "nuer_vocab_size = len(nuer_tokenizer.word_index)\n",
    "\n",
    "print('Data Preprocessed')\n",
    "print(\"Max English sentence length:\", max_english_sequence_length)\n",
    "print(\"Max Nuer sentence length:\", max_nuer_sequence_length)\n",
    "print(\"English vocabulary size:\", english_vocab_size)\n",
    "print(\"Nuer vocabulary size:\", nuer_vocab_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "The model architecture is Simple RNN, RNN with Embedding, Bidirectional RNN and Encoder-Decpder RNN.\n",
    "- First the neural network will be translating the input to words ids, and then a logits_to_text function will convert the ids(logits) from the neural network to the nuer translation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert logits to text function\n",
    "\n",
    "def logits_to_text(logits, tokenizer):\n",
    "    index_to_words = {id: word for word, id in tokenizer.word_index.items()}\n",
    "    index_to_words[0] = '<PAD>'\n",
    "    return ' '.join([index_to_words[prediction] for prediction in np.argmax(logits, 1)])\n",
    "\n",
    "print('`logits_to_text` function loaded.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
