{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nuer-English_Translation\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "The goal of this project is to create a translation model that translate english to nuer language(my native language). The model is going to translate a small set of data.\n",
    "\n",
    "## Approach\n",
    "\n",
    "To translate english to nuer, we need to build a recurrent neural network(RNN). To build the RNN pipeline we need to start by:\n",
    "1. **preprocessing**; Load and examine the data, clean, tokenize and pad it.\n",
    "2. **Modeling**; build, train, and test the model\n",
    "3. **Prediction**; Create specific translations of english to Nuer, and then compare the output translations to the ground truth translations.\n",
    "4. **Iteration**; Go through the model, experimenting with different architectures.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary packages and libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install the packages and libraries\n",
    "%pip install numpy\n",
    "%pip install tensorflow \n",
    "%pip install keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import load_func\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model, Sequential\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/makuachtenygatluak/Documents/alu-machine_learning/Nuer-English_Translation/transenv/bin/python\n",
      "['/Library/Frameworks/Python.framework/Versions/3.12/lib/python312.zip', '/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12', '/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/lib-dynload', '', '/Users/makuachtenygatluak/Documents/alu-machine_learning/Nuer-English_Translation/transenv/lib/python3.12/site-packages']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "- I will go through the dataset and clean it if necessary. I have two datasets, the english.txt and the nuer.txt files. Each line in the english.txt file has a respective translation in each line of nuer.txt. I created a function outside the notebook to load the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the data\n",
    "english_sentences = load_func.load_data('data/english.txt')\n",
    "nuer_sentences = load_func.load_data('data/nuer.txt')\n",
    "\n",
    "print(english_sentences[0])\n",
    "\n",
    "\n",
    "# Check the corresponding sentences\n",
    "for i in range(5):\n",
    "    print(\"English sample: \", english_sentences[i])\n",
    "    print(\"Nuer samples: \",nuer_sentences[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess\n",
    "- Convert the text into a sequences of integers using:\n",
    "1. Tokenization of the words into ids\n",
    "2. Adding padding to make all the sequences the samee length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tokenize the data\n",
    "def tokenize(x):\n",
    "    \"\"\" Tokenize x\n",
    "        : param x: List of sentences/strings to be tokenized\n",
    "        : return: Tuple of (tokenized x data, tokenizer used to tokenize x) \n",
    "    \"\"\"\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(x)\n",
    "    return tokenizer.texts_to_sequences(x), tokenizer\n",
    "\n",
    "\n",
    "text_sentences, text_tokenizer = tokenize(english_sentences)\n",
    "# print(text_sentences[0])\n",
    "\n",
    "for i, (sent, token_sent) in enumerate(zip(english_sentences, text_sentences)):\n",
    "    print('Sequence {} in x'.format(i + 1))\n",
    "    print('Original sentence:', sent)\n",
    "    print('Tokenized sentence:', token_sent)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad the data\n",
    "def pad(x, length=None):\n",
    "    \"\"\" Pad x\n",
    "        : param x: List of sequences.\n",
    "        : param length: Length to pad the sequence to.  If\n",
    "    \"\"\"\n",
    "    \n",
    "    return pad_sequences(x, maxlen=length, padding='post')\n",
    "text_sentences_padded = pad(text_sentences)\n",
    "for i, (token_sent, pad_sent) in enumerate(zip(text_sentences, text_sentences_padded)):\n",
    "    print('Sequence {} in x'.format(i + 1))\n",
    "    print('Original sentence:', token_sent)\n",
    "    print('Padded sentence:', pad_sent)\n",
    "    print()   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the data\n",
    "def preprocess(x, y):\n",
    "    \"\"\" Preprocess x and y\n",
    "        : param x: Feature List of sentences\n",
    "        : param y: Label List of sentences\n",
    "        : return: Tuple of (Preprocessed x, Preprocessed y, x tokenizer, y tokenizer)\n",
    "    \"\"\"\n",
    "    preprocess_x, x_tk = tokenize(x)\n",
    "    preprocess_y, y_tk = tokenize(y)\n",
    "\n",
    "    preprocess_x = pad(preprocess_x)\n",
    "    preprocess_y = pad(preprocess_y)\n",
    "\n",
    "    # Keras's sparse_categorical_crossentropy function requires the labels to be in 3 dimensions\n",
    "    preprocess_y = preprocess_y.reshape(*preprocess_y.shape, 1)\n",
    "\n",
    "    return preprocess_x, preprocess_y, x_tk, y_tk\n",
    "\n",
    "preproc_english_sentences, preproc_nuer_sentences, english_tokenizer, nuer_tokenizer = preprocess(english_sentences, nuer_sentences)\n",
    "\n",
    "max_english_sequence_length = preproc_english_sentences.shape[1]\n",
    "max_nuer_sequence_length = preproc_nuer_sentences.shape[1]\n",
    "english_vocab_size = len(english_tokenizer.word_index)\n",
    "nuer_vocab_size = len(nuer_tokenizer.word_index)\n",
    "\n",
    "print('Data Preprocessed')\n",
    "print(\"Max English sentence length:\", max_english_sequence_length)\n",
    "print(\"Max Nuer sentence length:\", max_nuer_sequence_length)\n",
    "print(\"English vocabulary size:\", english_vocab_size)\n",
    "print(\"Nuer vocabulary size:\", nuer_vocab_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "The model architecture is Simple RNN, RNN with Embedding, Bidirectional RNN and Encoder-Decpder RNN.\n",
    "- First the neural network will be translating the input to words ids, and then a logits_to_text function will convert the ids(logits) from the neural network to the nuer translation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert logits to text function\n",
    "\n",
    "def logits_to_text(logits, tokenizer):\n",
    "    index_to_words = {id: word for word, id in tokenizer.word_index.items()}\n",
    "    index_to_words[0] = '<PAD>'\n",
    "    return ' '.join([index_to_words[prediction] for prediction in np.argmax(logits, 1)])\n",
    "\n",
    "print('`logits_to_text` function loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple RNN model\n",
    "\n",
    "def simple_model(input_shape, output_sequence_length, english_vocab_size, nuer_vocab_size):\n",
    "    \"\"\"\n",
    "    Build and train a basic RNN on x and y\n",
    "    :param input_shape: Tuple of input shape\n",
    "    :param output_sequence_length: Length of output sequence\n",
    "    :param english_vocab_size: Number of unique English words in the dataset\n",
    "    :param nuer_vocab_size: Number of unique Nuer words in the dataset\n",
    "    :return: Keras model built, but not trained\n",
    "    \"\"\"\n",
    "    # Hyperparameters\n",
    "    learning_rate = 0.01\n",
    "    \n",
    "    # Build the layers\n",
    "    model = Sequential()\n",
    "    model.add(tf.keras.layers.SimpleRNN(english_vocab_size, input_shape=input_shape[1:], return_sequences=True))\n",
    "    model.add(tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(nuer_vocab_size, activation='softmax')))\n",
    "    \n",
    "    model.compile(loss=sparse_categorical_crossentropy,\n",
    "                  optimizer=Adam(learning_rate),\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Train the model\n",
    "# Reshape the input\n",
    "tmp_x = pad(preproc_english_sentences, max_nuer_sequence_length)\n",
    "tmp_x = tmp_x.reshape((-1, preproc_nuer_sentences.shape[-2], 1))\n",
    "\n",
    "# Train the neural network\n",
    "simple_rnn_model = simple_model(\n",
    "    tmp_x.shape,\n",
    "    max_nuer_sequence_length,\n",
    "    english_vocab_size,\n",
    "    nuer_vocab_size)\n",
    "\n",
    "simple_rnn_model.fit(tmp_x, preproc_nuer_sentences, batch_size=32, epochs=10, validation_split=0.2)\n",
    "\n",
    "# Print prediction(s)\n",
    "print(logits_to_text(simple_rnn_model.predict(tmp_x[:1])[0], nuer_tokenizer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print prediction(s)\n",
    "print(\"Prediction:\")\n",
    "print(logits_to_text(simple_rnn_model.predict(tmp_x[:1])[0], nuer_tokenizer))\n",
    "\n",
    "print(\"\\nCorrect Translation:\")\n",
    "print(nuer_sentences[:1])\n",
    "\n",
    "print(\"\\nOriginal text:\")\n",
    "print(english_sentences[:1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 2: Embedding\n",
    "\n",
    "def embed_model(input_shape, output_sequence_length, english_vocab_size, nuer_vocab_size):\n",
    "    \"\"\"\n",
    "    Build and train a RNN model using word embedding on x and y\n",
    "    :param input_shape: Tuple of input shape\n",
    "    :param output_sequence_length: Length of output sequence\n",
    "    :param english_vocab_size: Number of unique English words in the dataset\n",
    "    :param nuer_vocab_size: Number of unique Nuer words in the dataset\n",
    "    :return: Keras model built, but not trained\n",
    "    \"\"\"\n",
    "    # Hyperparameters\n",
    "    learning_rate = 0.001\n",
    "    \n",
    "    # Build the layers\n",
    "    model = Sequential()\n",
    "    model.add(tf.keras.layers.Embedding(english_vocab_size, 128, input_length=input_shape[1:][0], input_shape=input_shape[1:]))\n",
    "    model.add(tf.keras.layers.SimpleRNN(english_vocab_size, return_sequences=True))\n",
    "    model.add(tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(nuer_vocab_size, activation='softmax')))\n",
    "    \n",
    "    model.compile(loss=sparse_categorical_crossentropy,\n",
    "                  optimizer=Adam(learning_rate),\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Train the model\n",
    "# Reshape the input\n",
    "tmp_x = pad(preproc_english_sentences, max_nuer_sequence_length)\n",
    "tmp_x = tmp_x.reshape((-1, preproc_nuer_sentences.shape[-2]))\n",
    "\n",
    "# Train the neural network\n",
    "embed_rnn_model = embed_model(\n",
    "    tmp_x.shape,\n",
    "    max_nuer_sequence_length,\n",
    "    english_vocab_size,\n",
    "    nuer_vocab_size)\n",
    "\n",
    "embed_rnn_model.summary()\n",
    "\n",
    "embed_rnn_model.fit(tmp_x, preproc_nuer_sentences, batch_size=32, epochs=10, validation_split=0.2)\n",
    "\n",
    "# Print prediction(s)\n",
    "print(\"Prediction:\")\n",
    "print(logits_to_text(embed_rnn_model.predict(tmp_x[:1])[0], nuer_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print prediction(s)\n",
    "print(\"Prediction:\")\n",
    "print(logits_to_text(embed_rnn_model.predict(tmp_x[:1])[0], nuer_tokenizer))\n",
    "\n",
    "print(\"\\nCorrect Translation:\")\n",
    "print(nuer_sentences[:1])\n",
    "\n",
    "print(\"\\nOriginal text:\")\n",
    "print(english_sentences[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 3: Bidirectional RNNs\n",
    "\n",
    "def bd_model(input_shape, output_sequence_length, english_vocab_size, nuer_vocab_size):\n",
    "    \"\"\"\n",
    "    Build and train a bidirectional RNN model on x and y\n",
    "    :param input_shape: Tuple of input shape\n",
    "    :param output_sequence_length: Length of output sequence\n",
    "    :param english_vocab_size: Number of unique English words in the dataset\n",
    "    :param nuer_vocab_size: Number of unique Nuer words in the dataset\n",
    "    :return: Keras model built, but not trained\n",
    "    \"\"\"\n",
    "    # Hyperparameters\n",
    "    learning_rate = 0.001\n",
    "    \n",
    "    # Build the layers\n",
    "    model = Sequential()\n",
    "    model.add(tf.keras.layers.Bidirectional(tf.keras.layers.SimpleRNN(english_vocab_size, return_sequences=True), input_shape=input_shape[1:]))\n",
    "    model.add(tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(nuer_vocab_size, activation='softmax')))\n",
    "    \n",
    "    model.compile(loss=sparse_categorical_crossentropy,\n",
    "                  optimizer=Adam(learning_rate),\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Train the model\n",
    "# Reshape the input\n",
    "tmp_x = pad(preproc_english_sentences, max_nuer_sequence_length)\n",
    "tmp_x = tmp_x.reshape((-1, preproc_nuer_sentences.shape[-2], 1))\n",
    "\n",
    "# Train the neural network\n",
    "bd_rnn_model = bd_model(\n",
    "    tmp_x.shape,\n",
    "    max_nuer_sequence_length,\n",
    "    english_vocab_size,\n",
    "    nuer_vocab_size)\n",
    "\n",
    "bd_rnn_model.summary()\n",
    "bd_rnn_model.fit(tmp_x, preproc_nuer_sentences, batch_size=32, epochs=10, validation_split=0.2)\n",
    "\n",
    "# Print prediction(s)\n",
    "print(\"Prediction:\")\n",
    "print(logits_to_text(bd_rnn_model.predict(tmp_x[:1])[0], nuer_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print prediction(s)\n",
    "print(\"Prediction:\")\n",
    "print(logits_to_text(bd_rnn_model.predict(tmp_x[:1])[0], nuer_tokenizer))\n",
    "\n",
    "print(\"\\nCorrect Translation:\")\n",
    "print(nuer_sentences[:1])\n",
    "\n",
    "print(\"\\nOriginal text:\")\n",
    "print(english_sentences[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 4: Encoder-Decoder (RNN)\n",
    "\n",
    "def encdec_model(input_shape, output_sequence_length, english_vocab_size, nuer_vocab_size):\n",
    "    \"\"\"\n",
    "    Build and train an encoder-decoder model on x and y\n",
    "    :param input_shape: Tuple of input shape\n",
    "    :param output_sequence_length: Length of output sequence\n",
    "    :param english_vocab_size: Number of unique English words in the dataset\n",
    "    :param nuer_vocab_size: Number of unique Nuer words in the dataset\n",
    "    :return: Keras model built, but not trained\n",
    "    \"\"\"\n",
    "    # Hyperparameters\n",
    "    learning_rate = 0.001\n",
    "    \n",
    "    # Build the layers\n",
    "    model = Sequential()\n",
    "    model.add(tf.keras.layers.LSTM(english_vocab_size, input_shape=input_shape[1:], return_sequences=False))\n",
    "    model.add(tf.keras.layers.RepeatVector(output_sequence_length))\n",
    "    model.add(tf.keras.layers.LSTM(english_vocab_size, return_sequences=True))\n",
    "    model.add(tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(nuer_vocab_size, activation='softmax')))\n",
    "    \n",
    "    model.compile(loss=sparse_categorical_crossentropy,\n",
    "                  optimizer=Adam(learning_rate),\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Train the model\n",
    "# Reshape the input\n",
    "\n",
    "tmp_x = pad(preproc_english_sentences, max_nuer_sequence_length)\n",
    "tmp_x = tmp_x.reshape((-1, preproc_nuer_sentences.shape[-2], 1))\n",
    "\n",
    "# Train the neural network\n",
    "encdec_rnn_model = encdec_model(\n",
    "    tmp_x.shape,\n",
    "    max_nuer_sequence_length,\n",
    "    english_vocab_size,\n",
    "    nuer_vocab_size)\n",
    "\n",
    "encdec_rnn_model.summary()\n",
    "\n",
    "encdec_rnn_model.fit(tmp_x, preproc_nuer_sentences, batch_size=32, epochs=10, validation_split=0.2)\n",
    "\n",
    "# Print prediction(s)\n",
    "\n",
    "print(\"Prediction:\")\n",
    "print(logits_to_text(encdec_rnn_model.predict(tmp_x[:1])[0], nuer_tokenizer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print prediction(s)\n",
    "print(\"\\nCorrect Translation:\")\n",
    "print(nuer_sentences[:1])\n",
    "\n",
    "print(\"\\nOriginal text:\")\n",
    "print(english_sentences[:1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 5: Encoder-Decoder (RNN) with Embedding\n",
    "\n",
    "def encdec_embed_model(input_shape, output_sequence_length, english_vocab_size, nuer_vocab_size):\n",
    "    \"\"\"\n",
    "    Build and train an encoder-decoder model on x and y\n",
    "    :param input_shape: Tuple of input shape\n",
    "    :param output_sequence_length: Length of output sequence\n",
    "    :param english_vocab_size: Number of unique English words in the dataset\n",
    "    :param nuer_vocab_size: Number of unique Nuer words in the dataset\n",
    "    :return: Keras model built, but not trained\n",
    "    \"\"\"\n",
    "    # Hyperparameters\n",
    "    learning_rate = 0.001\n",
    "    \n",
    "    # Build the layers\n",
    "    model = Sequential()\n",
    "    model.add(tf.keras.layers.Embedding(english_vocab_size, 128, input_length=input_shape[1:][0], input_shape=input_shape[1:]))\n",
    "    model.add(tf.keras.layers.LSTM(english_vocab_size, return_sequences=False))\n",
    "    model.add(tf.keras.layers.RepeatVector(output_sequence_length))\n",
    "    model.add(tf.keras.layers.LSTM(english_vocab_size, return_sequences=True))\n",
    "    model.add(tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(nuer_vocab_size, activation='softmax')))\n",
    "    \n",
    "    model.compile(loss=sparse_categorical_crossentropy,\n",
    "                  optimizer=Adam(learning_rate),\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Train the model\n",
    "# Reshape the input\n",
    "tmp_x = pad(preproc_english_sentences, max_nuer_sequence_length)\n",
    "tmp_x = tmp_x.reshape((-1, preproc_nuer_sentences.shape[-2]))\n",
    "\n",
    "# Train the neural network\n",
    "encdec_embed_model = encdec_embed_model(\n",
    "    tmp_x.shape,\n",
    "    max_nuer_sequence_length,\n",
    "    english_vocab_size,\n",
    "    nuer_vocab_size)\n",
    "\n",
    "encdec_embed_model.summary()\n",
    "\n",
    "encdec_embed_model.fit(tmp_x, preproc_nuer_sentences, batch_size=32, epochs=10, validation_split=0.2)\n",
    "\n",
    "# Print prediction(s)\n",
    "print(\"Prediction:\")\n",
    "print(logits_to_text(encdec_embed_model.predict(tmp_x[:1])[0], nuer_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction implementation\n",
    "\n",
    "def final_predictions(x, y, x_tk, y_tk):\n",
    "    \"\"\"\n",
    "    Gets predictions using the final model\n",
    "    :param x: Preprocessed English data\n",
    "    :param y: Preprocessed Nuer data\n",
    "    :param x_tk: English tokenizer\n",
    "    :param y_tk: Nuer tokenizer\n",
    "    \"\"\"\n",
    "    # Train the neural network\n",
    "    model = encdec_embed_model(\n",
    "        x.shape,\n",
    "        y.shape[1],\n",
    "        len(x_tk.word_index)+1,\n",
    "        len(y_tk.word_index)+1)\n",
    "    model.fit(x, y, batch_size=32, epochs=10, validation_split=0.2)\n",
    "\n",
    "    ## Print prediction(s)\n",
    "    print(\"Prediction:\")\n",
    "    print(logits_to_text(model.predict(x[:1])[0], y_tk))\n",
    "\n",
    "    print(\"\\nCorrect Translation:\")\n",
    "    print(logits_to_text(y[:1][0], y_tk))\n",
    "\n",
    "    print(\"\\nOriginal text:\")\n",
    "    print(logits_to_text(x[:1][0], x_tk))\n",
    "    \n",
    "final_predictions(preproc_english_sentences, preproc_nuer_sentences, english_tokenizer, nuer_tokenizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimal enhancements\n",
    "\n",
    "some of the optimal enhancements that can be implemented are:\n",
    "1. Increase the number of epochs\n",
    "2. Increase the batch size\n",
    "3. Increase the number of layers\n",
    "4. Increase the number of units\n",
    "5. Increase the number of cells\n",
    "6. split the data into training and testing data\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
